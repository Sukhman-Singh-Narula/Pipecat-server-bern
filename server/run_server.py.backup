#!/usr/bin/env python3

"""
Enhanced Pipecat server with FastAPI, WebRTC, and Firebase integration
Works exactly like 07-interruptible.py but with dynamic system prompts
"""

import os
import asyncio
import uvicorn
import argparse
import sys
import importlib.util
import aiohttp
from contextlib import asynccontextmanager
from datetime import datetime, timezone
from typing import Dict, Any, Optional

from dotenv import load_dotenv
from loguru import logger
from fastapi import FastAPI, HTTPException, Request, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles

# Pipecat imports - exactly like 07-interruptible.py
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.runner.types import RunnerArguments, SmallWebRTCRunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.elevenlabs.tts import ElevenLabsHttpTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.network.fastapi_websocket import FastAPIWebsocketParams
from pipecat.transports.services.daily import DailyParams

# Import our enhanced functionality
from config.settings import get_settings, validate_settings
from services.firebase_service import FirebaseService
from utils import setup_logging, handle_generic_error

# Import the new comprehensive API routers
from api.enhanced_users import router as enhanced_users_router
from api.episodes import router as episodes_router
from api.conversations import router as conversations_router
from routes.prompts import router as prompts_router

load_dotenv(override=True)

# Global state for managing active sessions - like 07-interruptible.py
active_sessions: Dict[str, Any] = {}
active_transports: Dict[str, BaseTransport] = {}

# Transport params - exactly like 07-interruptible.py
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
}

def get_default_system_prompt() -> str:
    """Default system prompt - exactly like 07-interruptible.py"""
    return "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way."

async def get_enhanced_system_prompt(device_id: str = None) -> str:
    """Get enhanced system prompt based on user data from Firebase"""
    
    if not device_id:
        return get_default_system_prompt()
    
    try:
        # Get Firebase service
        from services.firebase_service import get_firebase_service
        from services.prompt_service import get_prompt_service
        
        firebase_service = get_firebase_service()
        prompt_service = get_prompt_service()
        
        # Get user document directly using device_id as document ID
        user_doc = await firebase_service.get_document("users", device_id)
        
        if user_doc:
            # Extract user data from document
            name = user_doc.get('name', 'Student')
            age = user_doc.get('age', 10)
            progress = user_doc.get('progress', {})
            
            # Get current season and episode
            season = progress.get('season', 1)
            episode = progress.get('episode', 1)
            episodes_completed = progress.get('episodes_completed', 0)
            words_learnt = progress.get('words_learnt', [])
            topics_learnt = progress.get('topics_learnt', [])
            
            logger.info(f"Found user for device {device_id}: {name} (age {age}, Season {season}, Episode {episode})")
            
            # Use PromptService to get fresh prompt data (no caching)
            try:
                prompt_response = await prompt_service.get_system_prompt(season, episode)
                
                # Extract system prompt data from PromptService response
                title = prompt_response.title
                content = prompt_response.prompt
                learning_objectives = prompt_response.metadata.get('learning_objectives', [])
                words_to_teach = prompt_response.metadata.get('words_to_teach', [])
                topics_to_cover = prompt_response.metadata.get('topics_to_cover', [])
                difficulty_level = prompt_response.metadata.get('difficulty_level', 'beginner')
                
                # Create enhanced system prompt with user context
                enhanced_prompt = f"""You are a friendly AI tutor helping {name} (age {age}) learn English.

Current Episode: {title}
Learning Level: {difficulty_level}
Learning Objectives: {', '.join(learning_objectives) if learning_objectives else 'General English conversation'}
Words to Teach: {', '.join(words_to_teach) if words_to_teach else 'Context-appropriate vocabulary'}
Topics to Cover: {', '.join(topics_to_cover) if topics_to_cover else 'General conversation topics'}

Episode Content:
{content}

Student Context:
- Name: {name}
- Age: {age}
- Device ID: {device_id}
- Current Progress: Season {season}, Episode {episode}
- Episodes Completed: {episodes_completed}
- Words Already Learned: {len(words_learnt)} words ({', '.join(words_learnt[-5:]) if words_learnt else 'none yet'})
- Topics Already Covered: {len(topics_learnt)} topics ({', '.join(topics_learnt[-3:]) if topics_learnt else 'none yet'})

Remember to:
- Use age-appropriate language for a {age}-year-old
- Focus on teaching these specific words: {', '.join(words_to_teach) if words_to_teach else 'vocabulary that comes up naturally'}
- Cover these topics naturally in conversation: {', '.join(topics_to_cover) if topics_to_cover else 'topics of interest to the student'}
- Build on {name}'s previous learning (they've completed {episodes_completed} episodes)
- Keep conversations engaging and interactive
- Provide gentle corrections and encouragement
- Adapt to {name}'s learning pace and interests
- Your output will be converted to audio, so avoid special characters

Start the conversation by greeting {name} warmly and beginning the lesson content."""
                
                logger.info(f"Created enhanced system prompt for {name} using PromptService - Length: {len(enhanced_prompt)} characters")
                return enhanced_prompt
                
            except Exception as prompt_error:
                logger.warning(f"Could not get prompt via PromptService for S{season}E{episode}: {prompt_error}")
                # Fall back to manual Firebase query (your original method)
                pass
                # No system prompt found, but we have user data
                logger.warning(f"No system prompt found for Season {season}, Episode {episode}. Using user-specific fallback.")
                
                return f"""You are a friendly AI tutor helping {name} (age {age}) learn English.

Current Progress: Season {season}, Episode {episode}
Previous Learning: {name} has completed {episodes_completed} episodes and learned {len(words_learnt)} words.

Since no specific lesson content is available, please:
- Greet {name} warmly by name
- Review some of the words they've learned: {', '.join(words_learnt[-5:]) if words_learnt else 'basic vocabulary'}
- Have a conversation appropriate for a {age}-year-old
- Introduce some new age-appropriate vocabulary
- Keep the lesson engaging and interactive
- Provide gentle corrections and encouragement
- Your output will be converted to audio, so avoid special characters

Start by asking {name} how they're feeling today and what they'd like to talk about."""
        
        else:
            logger.warning(f"No user document found for device {device_id}")
            # Fallback to device-specific but generic prompt
            return f"""You are a friendly AI tutor helping a student learn English.

Device: {device_id}

Please introduce yourself and start a conversational English lesson.

Remember to:
- Use simple, clear language
- Be encouraging and patient
- Ask questions to keep the conversation interactive
- Provide gentle corrections when needed
- Make learning fun and engaging
- Your output will be converted to audio, so avoid special characters

Start by asking the student their name and what they'd like to learn about today."""
    
    except Exception as e:
        logger.error(f"Error creating enhanced system prompt for {device_id}: {e}")
        # Ultimate fallback
        return get_default_system_prompt()

async def get_system_prompt_for_user(device_id: str) -> str:
    """Get user-specific system prompt based on their season/episode progress"""
    return await get_enhanced_system_prompt(device_id)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager"""
    # Startup
    logger.info("üöÄ Enhanced Pipecat Server starting up...")
    
    # Validate settings on startup
    settings = get_settings()
    validate_settings()
    
    logger.info(f"‚úÖ Server configured - App: {settings.app_name} v{settings.app_version}")
    logger.info(f"üìä Debug mode: {settings.debug}")
    
    # Initialize services
    firebase_service = FirebaseService()
    if hasattr(firebase_service, 'use_firebase') and firebase_service.use_firebase:
        logger.info("üî• Firebase integration enabled")
    else:
        logger.info("üíæ Using local storage (Firebase disabled)")
    
    yield
    
    # Shutdown
    logger.info("üëã Enhanced Pipecat Server shutting down...")

def create_enhanced_app() -> FastAPI:
    """Create enhanced FastAPI application with all features"""
    
    settings = get_settings()
    
    app = FastAPI(
        title="Enhanced Pipecat Server",
        version="1.0.0",
        description="""
        ## Enhanced Pipecat Server with User Management
        
        A comprehensive Pipecat server with:
        - **WebRTC Audio**: Real-time audio streaming with ESP32 devices
        - **User Management**: Device registration and progress tracking  
        - **Episode System**: Season/episode progression with custom prompts
        - **Firebase Integration**: User data and prompt storage
        - **Learning Analytics**: Progress tracking and statistics
        
        ### Getting Started:
        1. Register a user with POST /auth/register
        2. Upload system prompts with POST /prompts/
        3. Connect ESP32 via WebRTC audio at /client
        4. Monitor progress with GET /users/{device_id}
        """,
        docs_url="/docs",
        redoc_url="/redoc",
        lifespan=lifespan,
        debug=settings.debug
    )
    
    # Add CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Include the new comprehensive API routers
    app.include_router(enhanced_users_router, tags=["Enhanced Users"])
    app.include_router(episodes_router, tags=["Episode Prompts"])
    app.include_router(conversations_router, tags=["Conversations"])
    app.include_router(prompts_router, tags=["Prompts"])
    
    # Enhanced root endpoint
    @app.get("/", 
             summary="Server status",
             description="Get enhanced server status and information")
    async def root():
        """Root endpoint returning enhanced server status"""
        settings = get_settings()
        
        return {
            "message": "Enhanced Pipecat Server is running",
            "version": "1.0.0",
            "status": "healthy",
            "features": [
                "WebRTC audio streaming",
                "User registration and management",
                "Episode-based system prompts",
                "Learning progress tracking",
                "Firebase data storage",
                "ESP32 device support"
            ],
            "endpoints": {
                "documentation": "/docs",
                "webrtc_client": "/client",
                "health_check": "/health",
                "user_auth": "/auth/register",
                "user_management": "/users/{device_id}",
                "prompt_management": "/prompts/"
            },
            "configuration": {
                "app_name": settings.app_name,
                "version": settings.app_version,
                "debug": settings.debug
            }
        }
    
    # Health check endpoint
    @app.get("/health",
             summary="Health check",
             description="Check server health and status with ESP32 support")
    async def health_check():
        """Health check endpoint"""
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "esp32_mode": ESP32_MODE,
            "esp32_host": ESP32_HOST if ESP32_MODE else None,
            "sdp_munging": "enabled" if ESP32_MODE else "disabled",
            "services": {
                "firebase": "available",
                "enhanced_users": "running", 
                "episodes": "running",
                "conversations": "running",
                "openai": "available" if os.getenv("OPENAI_API_KEY") else "missing",
                "deepgram": "available" if os.getenv("DEEPGRAM_API_KEY") else "missing",
                "cartesia": "available" if os.getenv("CARTESIA_API_KEY") else "missing"
            }
        }

    # Import WebRTC static files and client from Pipecat
    from fastapi.staticfiles import StaticFiles
    from pathlib import Path
    
    # Add WebRTC client interface (mirroring what bot.py does)
    try:
        # Try to find the WebRTC client dist directory in common locations
        potential_paths = [
            # ESP-IDF environment paths
            "/Users/sukhmansinghnarula/.espressif/python_env/idf6.0_py3.13_env/lib/python3.13/site-packages/pipecat_ai_small_webrtc_prebuilt/client/dist",
            # Standard Python package paths for your server environment
            "/root/Pipecat-server-bern/venv/lib/python3.12/site-packages/pipecat_ai_small_webrtc_prebuilt/client/dist",
            "/usr/local/lib/python3.12/site-packages/pipecat_ai_small_webrtc_prebuilt/client/dist",
            # Try to find via site-packages dynamically
            f"{sys.prefix}/lib/python3.12/site-packages/pipecat_ai_small_webrtc_prebuilt/client/dist",
            f"{sys.prefix}/lib/python{sys.version_info.major}.{sys.version_info.minor}/site-packages/pipecat_ai_small_webrtc_prebuilt/client/dist",
            # Local paths
            "./client/dist",
            "../client/dist",
            "./static/client",
            "../static/client"
        ]
        
        # Try to get path from importlib (Python 3.9+)
        try:
            import importlib.util
            spec = importlib.util.find_spec('pipecat_ai_small_webrtc_prebuilt')
            if spec and spec.origin:
                pkg_dir = os.path.dirname(spec.origin)
                client_path = os.path.join(pkg_dir, 'client', 'dist')
                potential_paths.insert(0, client_path)
                logger.info(f"Added importlib-detected path: {client_path}")
        except Exception as e:
            logger.warning(f"Could not detect package path via importlib: {e}")
            pass
        
        dist_dir = None
        for path in potential_paths:
            abs_path = os.path.abspath(path)
            if Path(abs_path).exists():
                dist_dir = abs_path
                logger.info(f"Found WebRTC client dist directory at: {dist_dir}")
                break
        
        if dist_dir:
            # Mount the official WebRTC client at /client/original for reference
            app.mount("/client/original", StaticFiles(directory=dist_dir, html=True), name="webrtc_client_original")
            logger.info("‚úÖ Original WebRTC client interface mounted at /client/original")
            
            # Create our custom prompt client as the main /client endpoint
            @app.get("/client", response_class=HTMLResponse)
            async def client_with_custom_prompts():
                return HTMLResponse(content="""
                <!DOCTYPE html>
                <html lang="en">
                <head>
                    <meta charset="UTF-8">
                    <meta name="viewport" content="width=device-width, initial-scale=1.0">
                    <title>AI Voice Assistant - Custom Prompts</title>
                    <style>
                        body { font-family: Arial, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; background-color: #f5f5f5; }
                        .container { background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
                        .form-group { margin-bottom: 20px; }
                        label { display: block; margin-bottom: 5px; font-weight: bold; color: #333; }
                        input, textarea, button { width: 100%; padding: 12px; border: 1px solid #ddd; border-radius: 5px; font-size: 16px; }
                        textarea { min-height: 120px; resize: vertical; font-family: inherit; }
                        button { background: #007bff; color: white; cursor: pointer; margin-top: 10px; border: none; }
                        button:hover { background: #0056b3; }
                        button:disabled { background: #ccc; cursor: not-allowed; }
                        .status { padding: 12px; border-radius: 5px; margin: 15px 0; }
                        .success { background: #d4edda; color: #155724; border: 1px solid #c3e6cb; }
                        .error { background: #f8d7da; color: #721c24; border: 1px solid #f5c6cb; }
                        .warning { background: #fff3cd; color: #856404; border: 1px solid #ffeaa7; }
                        .info { background: #d1ecf1; color: #0c5460; border: 1px solid #bee5eb; }
                        #audioLevel { width: 100%; height: 25px; background: #f0f0f0; border: 1px solid #ccc; border-radius: 5px; margin-top: 10px; }
                        #audioLevelBar { height: 100%; background: linear-gradient(90deg, #28a745, #ffc107, #dc3545); width: 0%; transition: width 0.1s; border-radius: 5px; }
                        .logs { background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 5px; padding: 15px; height: 200px; overflow-y: auto; font-family: 'Courier New', monospace; font-size: 12px; }
                        .preset-prompts { display: flex; gap: 10px; flex-wrap: wrap; margin-bottom: 15px; }
                        .preset-btn { background: #6c757d; color: white; border: none; padding: 8px 16px; border-radius: 20px; cursor: pointer; font-size: 14px; }
                        .preset-btn:hover { background: #5a6268; }
                        .header { text-align: center; margin-bottom: 30px; }
                        .header h1 { color: #007bff; margin-bottom: 10px; }
                        .two-column { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }
                        @media (max-width: 768px) { .two-column { grid-template-columns: 1fr; } }
                        .note { background: #e7f3ff; border: 1px solid #bee5eb; border-radius: 5px; padding: 10px; margin-bottom: 20px; font-size: 14px; }
                    </style>
                </head>
                <body>
                    <div class="container">
                        <div class="header">
                            <h1>üéôÔ∏è AI Voice Assistant - Custom Prompts</h1>
                            <p>Customize your AI's personality and test voice conversations</p>
                        </div>
                        
                        <div class="note">
                            <strong>‚ú® Enhanced Version:</strong> This client supports custom system prompts. For the original WebRTC client, visit <a href="/client/original">/client/original</a>
                        </div>
                        
                        <div class="form-group">
                            <label for="deviceId">Device ID (Optional):</label>
                            <input type="text" id="deviceId" value="CUSTOM_CLIENT" placeholder="Enter device identifier">
                        </div>
                        
                        <div class="form-group">
                            <label for="systemPrompt">AI System Prompt:</label>
                            <div class="preset-prompts">
                                <button class="preset-btn" onclick="loadPreset('teacher')">üë©‚Äçüè´ Teacher</button>
                                <button class="preset-btn" onclick="loadPreset('friend')">üòä Friend</button>
                                <button class="preset-btn" onclick="loadPreset('coach')">üí™ Coach</button>
                                <button class="preset-btn" onclick="loadPreset('assistant')">ü§ñ Assistant</button>
                                <button class="preset-btn" onclick="loadPreset('storyteller')">üìö Storyteller</button>
                            </div>
                            <textarea id="systemPrompt" placeholder="Enter your custom system prompt here...">You are a helpful AI assistant in a voice conversation. Respond naturally and conversationally. Keep your responses concise since they will be spoken aloud. Be friendly, helpful, and engaging.</textarea>
                        </div>
                        
                        <div class="two-column">
                            <div>
                                <div class="form-group">
                                    <button onclick="testMicrophone()">üé§ Test Microphone</button>
                                    <div id="audioLevel">
                                        <div id="audioLevelBar"></div>
                                    </div>
                                    <small>Speak to see audio levels</small>
                                </div>
                            </div>
                            <div>
                                <div class="form-group">
                                    <button onclick="connectWebRTC()" id="connectBtn">üîó Start Voice Chat</button>
                                    <button onclick="disconnect()" id="disconnectBtn" disabled>‚ùå Stop Chat</button>
                                </div>
                            </div>
                        </div>
                        
                        <div id="status"></div>
                        
                        <h3>üìã Connection Logs:</h3>
                        <div id="logs" class="logs"></div>
                    </div>

                    <script>
                        let pc = null;
                        let localStream = null;
                        let audioContext = null;
                        let analyser = null;
                        let microphone = null;
                        let connected = false;

                        const presetPrompts = {
                            teacher: "You are a friendly and patient teacher having a voice conversation with a student. Explain concepts clearly, ask engaging questions, and provide encouragement. Keep responses educational but conversational since they will be spoken aloud.",
                            friend: "You are a warm, supportive friend having a casual voice chat. Be empathetic, ask follow-up questions, share appropriate stories or experiences, and keep the conversation light and engaging. Respond naturally as if talking to a close friend.",
                            coach: "You are an enthusiastic life coach and motivator in a voice conversation. Be encouraging, ask powerful questions, help the person think through challenges, and provide actionable advice. Keep your energy positive and inspiring.",
                            assistant: "You are a professional AI assistant in a voice conversation. Be helpful, efficient, and informative while maintaining a friendly tone. Provide clear, actionable responses and ask clarifying questions when needed.",
                            storyteller: "You are a captivating storyteller sharing tales through voice. Create engaging narratives, use vivid descriptions, vary your pacing for dramatic effect, and invite the listener into the story. Make it interactive by asking what they'd like to hear about."
                        };

                        function loadPreset(type) {
                            document.getElementById('systemPrompt').value = presetPrompts[type];
                            log(`Loaded ${type} preset prompt`, 'info');
                        }

                        function log(message, type = 'info') {
                            const logs = document.getElementById('logs');
                            const timestamp = new Date().toLocaleTimeString();
                            const logEntry = document.createElement('div');
                            logEntry.style.color = type === 'error' ? '#dc3545' : type === 'success' ? '#28a745' : type === 'warning' ? '#ffc107' : '#007bff';
                            logEntry.textContent = `[${timestamp}] ${message}`;
                            logs.appendChild(logEntry);
                            logs.scrollTop = logs.scrollHeight;
                            console.log(`[${type.toUpperCase()}] ${message}`);
                        }

                        function showStatus(message, type) {
                            const status = document.getElementById('status');
                            status.innerHTML = `<div class="${type}">${message}</div>`;
                        }

                        async function testMicrophone() {
                            try {
                                log('Requesting microphone access...', 'info');
                                
                                const stream = await navigator.mediaDevices.getUserMedia({
                                    audio: {
                                        echoCancellation: true,
                                        noiseSuppression: true,
                                        autoGainControl: true,
                                        sampleRate: 48000
                                    }
                                });

                                log('‚úÖ Microphone access granted!', 'success');
                                showStatus('‚úÖ Microphone ready! Speak to test audio levels.', 'success');

                                // Set up audio level monitoring
                                audioContext = new AudioContext();
                                analyser = audioContext.createAnalyser();
                                microphone = audioContext.createMediaStreamSource(stream);
                                
                                microphone.connect(analyser);
                                analyser.fftSize = 256;
                                
                                const bufferLength = analyser.frequencyBinCount;
                                const dataArray = new Uint8Array(bufferLength);

                                function updateAudioLevel() {
                                    analyser.getByteFrequencyData(dataArray);
                                    let sum = 0;
                                    for (let i = 0; i < bufferLength; i++) {
                                        sum += dataArray[i];
                                    }
                                    const average = sum / bufferLength;
                                    const percentage = Math.min((average / 128) * 100, 100);
                                    
                                    document.getElementById('audioLevelBar').style.width = percentage + '%';
                                    
                                    if (!connected) {
                                        requestAnimationFrame(updateAudioLevel);
                                    }
                                }
                                updateAudioLevel();

                                localStream = stream;
                                
                            } catch (error) {
                                log(`‚ùå Microphone access failed: ${error.message}`, 'error');
                                showStatus(`‚ùå Microphone access denied. Please allow microphone permissions and try again.`, 'error');
                            }
                        }

                        async function connectWebRTC() {
                            const deviceId = document.getElementById('deviceId').value.trim() || 'CUSTOM_CLIENT';
                            const systemPrompt = document.getElementById('systemPrompt').value.trim();
                            
                            if (!systemPrompt) {
                                showStatus('‚ùå Please enter a system prompt', 'error');
                                return;
                            }

                            if (!localStream) {
                                showStatus('‚ùå Please test microphone first', 'error');
                                return;
                            }

                            try {
                                document.getElementById('connectBtn').disabled = true;
                                log(`Starting voice chat with device: ${deviceId}`, 'info');
                                log(`Using custom system prompt (${systemPrompt.length} characters)`, 'info');

                                pc = new RTCPeerConnection({
                                    iceServers: [
                                        { urls: 'stun:stun.l.google.com:19302' },
                                        { urls: 'stun:stun1.l.google.com:19302' }
                                    ]
                                });

                                localStream.getTracks().forEach(track => {
                                    pc.addTrack(track, localStream);
                                    log(`Added ${track.kind} track`, 'info');
                                });

                                pc.ontrack = (event) => {
                                    log(`Received ${event.track.kind} from AI`, 'success');
                                    if (event.track.kind === 'audio') {
                                        const audio = document.createElement('audio');
                                        audio.srcObject = event.streams[0];
                                        audio.autoplay = true;
                                        audio.style.display = 'none';
                                        document.body.appendChild(audio);
                                        log('üîä AI voice channel ready', 'success');
                                    }
                                };

                                pc.onicecandidate = (event) => {
                                    if (event.candidate) {
                                        log(`ICE: ${event.candidate.candidate.split(' ')[7] || 'candidate'}`, 'info');
                                    }
                                };

                                pc.onconnectionstatechange = () => {
                                    log(`Connection: ${pc.connectionState}`, 'info');
                                    if (pc.connectionState === 'connected') {
                                        connected = true;
                                        showStatus('‚úÖ Voice chat connected! Start speaking to the AI.', 'success');
                                        document.getElementById('disconnectBtn').disabled = false;
                                    } else if (pc.connectionState === 'failed') {
                                        showStatus('‚ùå Connection failed. Try again.', 'error');
                                        disconnect();
                                    }
                                };

                                const offer = await pc.createOffer({
                                    offerToReceiveAudio: true,
                                    offerToReceiveVideo: false
                                });
                                
                                await pc.setLocalDescription(offer);
                                log('Created WebRTC offer', 'info');

                                const response = await fetch('/api/offer', {
                                    method: 'POST',
                                    headers: {
                                        'Content-Type': 'application/json',
                                        'X-Device-ID': deviceId,
                                        'X-Custom-Prompt': 'true'
                                    },
                                    body: JSON.stringify({
                                        device_id: deviceId,
                                        type: 'offer',
                                        sdp: offer.sdp,
                                        custom_system_prompt: systemPrompt
                                    })
                                });

                                if (!response.ok) {
                                    throw new Error(`Server error: ${response.status}`);
                                }

                                const answer = await response.json();
                                log('Received server response', 'success');

                                await pc.setRemoteDescription(new RTCSessionDescription({
                                    type: 'answer',
                                    sdp: answer.sdp
                                }));
                                
                                log('WebRTC handshake completed', 'success');
                                showStatus('üîÑ Connecting... The AI will introduce itself shortly.', 'info');

                            } catch (error) {
                                log(`‚ùå Connection failed: ${error.message}`, 'error');
                                showStatus(`‚ùå Connection failed: ${error.message}`, 'error');
                                document.getElementById('connectBtn').disabled = false;
                            }
                        }

                        function disconnect() {
                            if (pc) {
                                pc.close();
                                pc = null;
                                log('WebRTC connection closed', 'info');
                            }
                            
                            if (localStream) {
                                localStream.getTracks().forEach(track => track.stop());
                                localStream = null;
                                log('Microphone stopped', 'info');
                            }

                            if (audioContext) {
                                audioContext.close();
                                audioContext = null;
                            }

                            connected = false;
                            document.getElementById('connectBtn').disabled = false;
                            document.getElementById('disconnectBtn').disabled = true;
                            document.getElementById('audioLevelBar').style.width = '0%';
                            showStatus('Voice chat disconnected', 'info');
                        }

                        // Load default preset on page load
                        window.addEventListener('load', () => {
                            log('üöÄ AI Voice Assistant with Custom Prompts ready', 'info');
                            log('1. Choose a preset or write custom prompt', 'info');
                            log('2. Test microphone', 'info');
                            log('3. Start voice chat', 'info');
                        });
                    </script>
                </body>
                </html>
                """)
            logger.info("‚úÖ Custom prompt client interface created as main /client endpoint")
            
            # Also add the /client/custom endpoint for consistency
            @app.get("/client/custom", response_class=HTMLResponse)
            async def client_custom():
                return HTMLResponse(content="""
                <!DOCTYPE html>
                <html>
                <head><title>Redirecting to Custom Client</title></head>
                <body>
                    <h1>Redirecting...</h1>
                    <p>The custom prompt client is now available at <a href="/client">/client</a></p>
                    <script>window.location.href = '/client';</script>
                </body>
                </html>
                """)
            logger.info("‚úÖ /client/custom redirects to main custom client")
        else:
            logger.warning("WebRTC client dist directory not found in any expected location")
            logger.info("Available paths checked:")
            for path in potential_paths:
                exists = "‚úÖ" if Path(path).exists() else "‚ùå"
                logger.info(f"  {exists} {path}")
            
            # Create a custom /client endpoint with system prompt input
            @app.get("/client", response_class=HTMLResponse)
            async def client_fallback():
                return HTMLResponse(content="""
                <!DOCTYPE html>
                <html lang="en">
                <head>
                    <meta charset="UTF-8">
                    <meta name="viewport" content="width=device-width, initial-scale=1.0">
                    <title>AI Voice Assistant - Custom Prompts</title>
                    <style>
                        body { font-family: Arial, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; background-color: #f5f5f5; }
                        .container { background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
                        .form-group { margin-bottom: 20px; }
                        label { display: block; margin-bottom: 5px; font-weight: bold; color: #333; }
                        input, textarea, button { width: 100%; padding: 12px; border: 1px solid #ddd; border-radius: 5px; font-size: 16px; }
                        textarea { min-height: 120px; resize: vertical; font-family: inherit; }
                        button { background: #007bff; color: white; cursor: pointer; margin-top: 10px; border: none; }
                        button:hover { background: #0056b3; }
                        button:disabled { background: #ccc; cursor: not-allowed; }
                        .status { padding: 12px; border-radius: 5px; margin: 15px 0; }
                        .success { background: #d4edda; color: #155724; border: 1px solid #c3e6cb; }
                        .error { background: #f8d7da; color: #721c24; border: 1px solid #f5c6cb; }
                        .warning { background: #fff3cd; color: #856404; border: 1px solid #ffeaa7; }
                        .info { background: #d1ecf1; color: #0c5460; border: 1px solid #bee5eb; }
                        #audioLevel { width: 100%; height: 25px; background: #f0f0f0; border: 1px solid #ccc; border-radius: 5px; margin-top: 10px; }
                        #audioLevelBar { height: 100%; background: linear-gradient(90deg, #28a745, #ffc107, #dc3545); width: 0%; transition: width 0.1s; border-radius: 5px; }
                        .logs { background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 5px; padding: 15px; height: 200px; overflow-y: auto; font-family: 'Courier New', monospace; font-size: 12px; }
                        .preset-prompts { display: flex; gap: 10px; flex-wrap: wrap; margin-bottom: 15px; }
                        .preset-btn { background: #6c757d; color: white; border: none; padding: 8px 16px; border-radius: 20px; cursor: pointer; font-size: 14px; }
                        .preset-btn:hover { background: #5a6268; }
                        .header { text-align: center; margin-bottom: 30px; }
                        .header h1 { color: #007bff; margin-bottom: 10px; }
                        .two-column { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }
                        @media (max-width: 768px) { .two-column { grid-template-columns: 1fr; } }
                    </style>
                </head>
                <body>
                    <div class="container">
                        <div class="header">
                            <h1>üéôÔ∏è AI Voice Assistant</h1>
                            <p>Customize your AI's personality and test voice conversations</p>
                        </div>
                        
                        <div class="form-group">
                            <label for="deviceId">Device ID (Optional):</label>
                            <input type="text" id="deviceId" value="CUSTOM_CLIENT" placeholder="Enter device identifier">
                        </div>
                        
                        <div class="form-group">
                            <label for="systemPrompt">AI System Prompt:</label>
                            <div class="preset-prompts">
                                <button class="preset-btn" onclick="loadPreset('teacher')">üë©‚Äçüè´ Teacher</button>
                                <button class="preset-btn" onclick="loadPreset('friend')">üòä Friend</button>
                                <button class="preset-btn" onclick="loadPreset('coach')">üí™ Coach</button>
                                <button class="preset-btn" onclick="loadPreset('assistant')">ü§ñ Assistant</button>
                                <button class="preset-btn" onclick="loadPreset('storyteller')">üìö Storyteller</button>
                            </div>
                            <textarea id="systemPrompt" placeholder="Enter your custom system prompt here...">You are a helpful AI assistant in a voice conversation. Respond naturally and conversationally. Keep your responses concise since they will be spoken aloud. Be friendly, helpful, and engaging.</textarea>
                        </div>
                        
                        <div class="two-column">
                            <div>
                                <div class="form-group">
                                    <button onclick="testMicrophone()">üé§ Test Microphone</button>
                                    <div id="audioLevel">
                                        <div id="audioLevelBar"></div>
                                    </div>
                                    <small>Speak to see audio levels</small>
                                </div>
                            </div>
                            <div>
                                <div class="form-group">
                                    <button onclick="connectWebRTC()" id="connectBtn">üîó Start Voice Chat</button>
                                    <button onclick="disconnect()" id="disconnectBtn" disabled>‚ùå Stop Chat</button>
                                </div>
                            </div>
                        </div>
                        
                        <div id="status"></div>
                        
                        <h3>üìã Connection Logs:</h3>
                        <div id="logs" class="logs"></div>
                    </div>

                    <script>
                        let pc = null;
                        let localStream = null;
                        let audioContext = null;
                        let analyser = null;
                        let microphone = null;
                        let connected = false;

                        const presetPrompts = {
                            teacher: "You are a friendly and patient teacher having a voice conversation with a student. Explain concepts clearly, ask engaging questions, and provide encouragement. Keep responses educational but conversational since they will be spoken aloud.",
                            friend: "You are a warm, supportive friend having a casual voice chat. Be empathetic, ask follow-up questions, share appropriate stories or experiences, and keep the conversation light and engaging. Respond naturally as if talking to a close friend.",
                            coach: "You are an enthusiastic life coach and motivator in a voice conversation. Be encouraging, ask powerful questions, help the person think through challenges, and provide actionable advice. Keep your energy positive and inspiring.",
                            assistant: "You are a professional AI assistant in a voice conversation. Be helpful, efficient, and informative while maintaining a friendly tone. Provide clear, actionable responses and ask clarifying questions when needed.",
                            storyteller: "You are a captivating storyteller sharing tales through voice. Create engaging narratives, use vivid descriptions, vary your pacing for dramatic effect, and invite the listener into the story. Make it interactive by asking what they'd like to hear about."
                        };

                        function loadPreset(type) {
                            document.getElementById('systemPrompt').value = presetPrompts[type];
                            log(`Loaded ${type} preset prompt`, 'info');
                        }

                        function log(message, type = 'info') {
                            const logs = document.getElementById('logs');
                            const timestamp = new Date().toLocaleTimeString();
                            const logEntry = document.createElement('div');
                            logEntry.style.color = type === 'error' ? '#dc3545' : type === 'success' ? '#28a745' : type === 'warning' ? '#ffc107' : '#007bff';
                            logEntry.textContent = `[${timestamp}] ${message}`;
                            logs.appendChild(logEntry);
                            logs.scrollTop = logs.scrollHeight;
                            console.log(`[${type.toUpperCase()}] ${message}`);
                        }

                        function showStatus(message, type) {
                            const status = document.getElementById('status');
                            status.innerHTML = `<div class="${type}">${message}</div>`;
                        }

                        async function testMicrophone() {
                            try {
                                log('Requesting microphone access...', 'info');
                                
                                const stream = await navigator.mediaDevices.getUserMedia({
                                    audio: {
                                        echoCancellation: true,
                                        noiseSuppression: true,
                                        autoGainControl: true,
                                        sampleRate: 48000
                                    }
                                });

                                log('‚úÖ Microphone access granted!', 'success');
                                showStatus('‚úÖ Microphone ready! Speak to test audio levels.', 'success');

                                // Set up audio level monitoring
                                audioContext = new AudioContext();
                                analyser = audioContext.createAnalyser();
                                microphone = audioContext.createMediaStreamSource(stream);
                                
                                microphone.connect(analyser);
                                analyser.fftSize = 256;
                                
                                const bufferLength = analyser.frequencyBinCount;
                                const dataArray = new Uint8Array(bufferLength);

                                function updateAudioLevel() {
                                    analyser.getByteFrequencyData(dataArray);
                                    let sum = 0;
                                    for (let i = 0; i < bufferLength; i++) {
                                        sum += dataArray[i];
                                    }
                                    const average = sum / bufferLength;
                                    const percentage = Math.min((average / 128) * 100, 100);
                                    
                                    document.getElementById('audioLevelBar').style.width = percentage + '%';
                                    
                                    if (!connected) {
                                        requestAnimationFrame(updateAudioLevel);
                                    }
                                }
                                updateAudioLevel();

                                localStream = stream;
                                
                            } catch (error) {
                                log(`‚ùå Microphone access failed: ${error.message}`, 'error');
                                showStatus(`‚ùå Microphone access denied. Please allow microphone permissions and try again.`, 'error');
                            }
                        }

                        async function connectWebRTC() {
                            const deviceId = document.getElementById('deviceId').value.trim() || 'CUSTOM_CLIENT';
                            const systemPrompt = document.getElementById('systemPrompt').value.trim();
                            
                            if (!systemPrompt) {
                                showStatus('‚ùå Please enter a system prompt', 'error');
                                return;
                            }

                            if (!localStream) {
                                showStatus('‚ùå Please test microphone first', 'error');
                                return;
                            }

                            try {
                                document.getElementById('connectBtn').disabled = true;
                                log(`Starting voice chat with device: ${deviceId}`, 'info');
                                log(`Using custom system prompt (${systemPrompt.length} characters)`, 'info');

                                pc = new RTCPeerConnection({
                                    iceServers: [
                                        { urls: 'stun:stun.l.google.com:19302' },
                                        { urls: 'stun:stun1.l.google.com:19302' }
                                    ]
                                });

                                localStream.getTracks().forEach(track => {
                                    pc.addTrack(track, localStream);
                                    log(`Added ${track.kind} track`, 'info');
                                });

                                pc.ontrack = (event) => {
                                    log(`Received ${event.track.kind} from AI`, 'success');
                                    if (event.track.kind === 'audio') {
                                        const audio = document.createElement('audio');
                                        audio.srcObject = event.streams[0];
                                        audio.autoplay = true;
                                        audio.style.display = 'none';
                                        document.body.appendChild(audio);
                                        log('üîä AI voice channel ready', 'success');
                                    }
                                };

                                pc.onicecandidate = (event) => {
                                    if (event.candidate) {
                                        log(`ICE: ${event.candidate.candidate.split(' ')[7] || 'candidate'}`, 'info');
                                    }
                                };

                                pc.onconnectionstatechange = () => {
                                    log(`Connection: ${pc.connectionState}`, 'info');
                                    if (pc.connectionState === 'connected') {
                                        connected = true;
                                        showStatus('‚úÖ Voice chat connected! Start speaking to the AI.', 'success');
                                        document.getElementById('disconnectBtn').disabled = false;
                                    } else if (pc.connectionState === 'failed') {
                                        showStatus('‚ùå Connection failed. Try again.', 'error');
                                        disconnect();
                                    }
                                };

                                const offer = await pc.createOffer({
                                    offerToReceiveAudio: true,
                                    offerToReceiveVideo: false
                                });
                                
                                await pc.setLocalDescription(offer);
                                log('Created WebRTC offer', 'info');

                                const response = await fetch('/api/offer', {
                                    method: 'POST',
                                    headers: {
                                        'Content-Type': 'application/json',
                                        'X-Device-ID': deviceId,
                                        'X-Custom-Prompt': 'true'
                                    },
                                    body: JSON.stringify({
                                        device_id: deviceId,
                                        type: 'offer',
                                        sdp: offer.sdp,
                                        custom_system_prompt: systemPrompt
                                    })
                                });

                                if (!response.ok) {
                                    throw new Error(`Server error: ${response.status}`);
                                }

                                const answer = await response.json();
                                log('Received server response', 'success');

                                await pc.setRemoteDescription(new RTCSessionDescription({
                                    type: 'answer',
                                    sdp: answer.sdp
                                }));
                                
                                log('WebRTC handshake completed', 'success');
                                showStatus('üîÑ Connecting... The AI will introduce itself shortly.', 'info');

                            } catch (error) {
                                log(`‚ùå Connection failed: ${error.message}`, 'error');
                                showStatus(`‚ùå Connection failed: ${error.message}`, 'error');
                                document.getElementById('connectBtn').disabled = false;
                            }
                        }

                        function disconnect() {
                            if (pc) {
                                pc.close();
                                pc = null;
                                log('WebRTC connection closed', 'info');
                            }
                            
                            if (localStream) {
                                localStream.getTracks().forEach(track => track.stop());
                                localStream = null;
                                log('Microphone stopped', 'info');
                            }

                            if (audioContext) {
                                audioContext.close();
                                audioContext = null;
                            }

                            connected = false;
                            document.getElementById('connectBtn').disabled = false;
                            document.getElementById('disconnectBtn').disabled = true;
                            document.getElementById('audioLevelBar').style.width = '0%';
                            showStatus('Voice chat disconnected', 'info');
                        }

                        // Load default preset on page load
                        window.addEventListener('load', () => {
                            log('üöÄ AI Voice Assistant ready', 'info');
                            log('1. Choose a preset or write custom prompt', 'info');
                            log('2. Test microphone', 'info');
                            log('3. Start voice chat', 'info');
                        });
                    </script>
                </body>
                </html>
                """)
            logger.info("‚úÖ Created custom /client endpoint with system prompt input")
            
    except Exception as e:
        logger.warning(f"Could not mount WebRTC client: {e}")
        
        # Create a fallback /client endpoint even on error
        @app.get("/client", response_class=HTMLResponse)
        async def client_error_fallback():
            return HTMLResponse(content="""
            <!DOCTYPE html>
            <html>
            <head><title>WebRTC Client - Error</title></head>
            <body>
                <h1>WebRTC Client Error</h1>
                <p>Could not load the official WebRTC client interface.</p>
                <p><a href="/test">Use the ESP32 test client instead</a></p>
                <p><a href="/docs">View API documentation</a></p>
            </body>
            </html>
            """)
        logger.info("‚úÖ Created error fallback /client endpoint")
    
    # Add test page for ESP32 debugging
    @app.get("/test", response_class=HTMLResponse)
    async def test_page():
        """Serve ESP32 WebRTC test page"""
        return HTMLResponse(content="""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ESP32 WebRTC Test - Fixed</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; background-color: #f5f5f5; }
        .container { background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .form-group { margin-bottom: 20px; }
        label { display: block; margin-bottom: 5px; font-weight: bold; }
        input, button { width: 100%; padding: 10px; border: 1px solid #ddd; border-radius: 5px; font-size: 16px; }
        button { background: #007bff; color: white; cursor: pointer; margin-top: 10px; }
        button:hover { background: #0056b3; }
        button:disabled { background: #ccc; cursor: not-allowed; }
        .status { padding: 10px; border-radius: 5px; margin: 10px 0; }
        .success { background: #d4edda; color: #155724; border: 1px solid #c3e6cb; }
        .error { background: #f8d7da; color: #721c24; border: 1px solid #f5c6cb; }
        .warning { background: #fff3cd; color: #856404; border: 1px solid #ffeaa7; }
        .info { background: #d1ecf1; color: #0c5460; border: 1px solid #bee5eb; }
        #audioLevel { width: 100%; height: 20px; background: #f0f0f0; border: 1px solid #ccc; margin-top: 10px; }
        #audioLevelBar { height: 100%; background: #4CAF50; width: 0%; transition: width 0.1s; }
        .logs { background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 5px; padding: 15px; height: 200px; overflow-y: auto; font-family: monospace; font-size: 12px; }
    </style>
</head>
<body>
    <div class="container">
        <h1>üéôÔ∏è ESP32 WebRTC Test Client</h1>
        <p>This page tests WebRTC connections to debug ESP32 communication issues.</p>
        
        <div class="form-group">
            <label for="deviceId">ESP32 Device ID:</label>
            <input type="text" id="deviceId" value="ESPX3001" placeholder="Enter your ESP32 device ID">
        </div>
        
        <div class="form-group">
            <button onclick="testMicrophone()">üé§ Test Microphone Access</button>
            <div id="audioLevel">
                <div id="audioLevelBar"></div>
            </div>
            <small>Audio level indicator (speak to test microphone)</small>
        </div>
        
        <div class="form-group">
            <button onclick="connectWebRTC()" id="connectBtn">üîó Start WebRTC Connection</button>
            <button onclick="disconnect()" id="disconnectBtn" disabled>‚ùå Disconnect</button>
        </div>
        
        <div id="status"></div>
        
        <h3>üìã Connection Logs:</h3>
        <div id="logs" class="logs"></div>
    </div>

    <script>
        let pc = null;
        let localStream = null;
        let audioContext = null;
        let analyser = null;
        let microphone = null;
        let connected = false;

        function log(message, type = 'info') {
            const logs = document.getElementById('logs');
            const timestamp = new Date().toLocaleTimeString();
            const logEntry = document.createElement('div');
            logEntry.style.color = type === 'error' ? 'red' : type === 'success' ? 'green' : type === 'warning' ? 'orange' : 'black';
            logEntry.textContent = `[${timestamp}] ${message}`;
            logs.appendChild(logEntry);
            logs.scrollTop = logs.scrollHeight;
            console.log(`[${type.toUpperCase()}] ${message}`);
        }

        function showStatus(message, type) {
            const status = document.getElementById('status');
            status.innerHTML = `<div class="${type}">${message}</div>`;
        }

        async function testMicrophone() {
            try {
                log('Testing microphone access...', 'info');
                
                // Request microphone permission with better constraints
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        sampleRate: 48000
                    }
                });

                log('‚úÖ Microphone access granted!', 'success');
                showStatus('‚úÖ Microphone access successful! Audio level monitoring started.', 'success');

                // Set up audio level monitoring
                audioContext = new AudioContext();
                analyser = audioContext.createAnalyser();
                microphone = audioContext.createMediaStreamSource(stream);
                
                microphone.connect(analyser);
                analyser.fftSize = 256;
                
                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);

                function updateAudioLevel() {
                    analyser.getByteFrequencyData(dataArray);
                    let sum = 0;
                    for (let i = 0; i < bufferLength; i++) {
                        sum += dataArray[i];
                    }
                    const average = sum / bufferLength;
                    const percentage = (average / 255) * 100;
                    
                    document.getElementById('audioLevelBar').style.width = percentage + '%';
                    
                    if (!connected) {
                        requestAnimationFrame(updateAudioLevel);
                    }
                }
                updateAudioLevel();

                // Store stream for WebRTC
                localStream = stream;
                
            } catch (error) {
                log(`‚ùå Microphone access failed: ${error.message}`, 'error');
                showStatus(`‚ùå Microphone access denied: ${error.message}`, 'error');
            }
        }

        async function connectWebRTC() {
            const deviceId = document.getElementById('deviceId').value.trim();
            if (!deviceId) {
                showStatus('‚ùå Please enter a device ID', 'error');
                return;
            }

            if (!localStream) {
                showStatus('‚ùå Please test microphone first', 'error');
                return;
            }

            try {
                document.getElementById('connectBtn').disabled = true;
                log(`Starting WebRTC connection for device: ${deviceId}`, 'info');

                // Create peer connection with proper configuration
                pc = new RTCPeerConnection({
                    iceServers: [
                        { urls: 'stun:stun.l.google.com:19302' }
                    ]
                });

                // Add local stream
                localStream.getTracks().forEach(track => {
                    pc.addTrack(track, localStream);
                    log(`Added ${track.kind} track to peer connection`, 'info');
                });

                // Handle remote stream
                pc.ontrack = (event) => {
                    log(`Received remote ${event.track.kind} track`, 'success');
                    if (event.track.kind === 'audio') {
                        const audio = document.createElement('audio');
                        audio.srcObject = event.streams[0];
                        audio.autoplay = true;
                        audio.controls = true;
                        document.body.appendChild(audio);
                        log('‚úÖ Audio element created for bot speech', 'success');
                    }
                };

                // Handle ICE candidates
                pc.onicecandidate = (event) => {
                    if (event.candidate) {
                        log(`ICE candidate: ${event.candidate.candidate}`, 'info');
                    } else {
                        log('All ICE candidates have been sent', 'info');
                    }
                };

                pc.onconnectionstatechange = () => {
                    log(`Connection state: ${pc.connectionState}`, 'info');
                    if (pc.connectionState === 'connected') {
                        connected = true;
                        showStatus('‚úÖ WebRTC connection established!', 'success');
                        document.getElementById('disconnectBtn').disabled = false;
                    } else if (pc.connectionState === 'failed') {
                        showStatus('‚ùå WebRTC connection failed', 'error');
                        disconnect();
                    }
                };

                // Create offer
                const offer = await pc.createOffer({
                    offerToReceiveAudio: true,
                    offerToReceiveVideo: false
                });
                
                await pc.setLocalDescription(offer);
                log('Created and set local offer', 'info');

                // Send offer to server with device ID
                const response = await fetch('/api/offer', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'X-Device-ID': deviceId
                    },
                    body: JSON.stringify({
                        device_id: deviceId,
                        type: 'offer',
                        sdp: offer.sdp
                    })
                });

                if (!response.ok) {
                    throw new Error(`HTTP ${response.status}: ${response.statusText}`);
                }

                const answer = await response.json();
                log('Received answer from server', 'success');

                await pc.setRemoteDescription(new RTCSessionDescription({
                    type: 'answer',
                    sdp: answer.sdp
                }));
                
                log('‚úÖ WebRTC handshake completed!', 'success');
                showStatus('üîÑ Connecting... Check for audio from the AI assistant.', 'info');

            } catch (error) {
                log(`‚ùå WebRTC connection failed: ${error.message}`, 'error');
                showStatus(`‚ùå Connection failed: ${error.message}`, 'error');
                document.getElementById('connectBtn').disabled = false;
            }
        }

        function disconnect() {
            if (pc) {
                pc.close();
                pc = null;
                log('WebRTC connection closed', 'info');
            }
            
            if (localStream) {
                localStream.getTracks().forEach(track => track.stop());
                localStream = null;
                log('Local media stream stopped', 'info');
            }

            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }

            connected = false;
            document.getElementById('connectBtn').disabled = false;
            document.getElementById('disconnectBtn').disabled = true;
            document.getElementById('audioLevelBar').style.width = '0%';
            showStatus('Connection closed', 'info');
        }

        // Auto-test microphone on page load
        window.addEventListener('load', () => {
            log('üöÄ ESP32 WebRTC Test Client loaded', 'info');
            log('Click "Test Microphone Access" first, then "Start WebRTC Connection"', 'info');
        });
    </script>
</body>
</html>
        """)
    
    # WebRTC offer endpoint - exactly like 07-interruptible.py approach with ESP32 support
    @app.post("/api/offer",
              summary="WebRTC offer handler", 
              description="Handle WebRTC offer from ESP32 devices with custom prompts and SDP munging")
    async def handle_webrtc_offer(request: Request, background_tasks: BackgroundTasks):
        """Handle WebRTC offers exactly like 07-interruptible.py but with Firebase integration and ESP32 support"""
        try:
            body = await request.json()
            
            # Extract device ID from multiple sources
            device_id = (
                body.get("device_id") or 
                request.headers.get("X-Device-ID") or
                request.query_params.get("device_id") or
                None
            )
            
            # Extract custom system prompt if provided
            custom_system_prompt = body.get("custom_system_prompt")
            is_custom_prompt = request.headers.get("X-Custom-Prompt") == "true"
            
            logger.info(f"Received WebRTC offer from device: {device_id}")
            if custom_system_prompt:
                logger.info(f"Using custom system prompt ({len(custom_system_prompt)} characters)")
            
            # Store session info with custom prompt info
            session_id = f"webrtc_{device_id or 'unknown'}_{len(active_sessions)}"
            active_sessions[session_id] = {
                "device_id": device_id,
                "created_at": datetime.now(timezone.utc),
                "status": "connecting",
                "type": "webrtc",
                "custom_prompt": bool(custom_system_prompt),
                "prompt_length": len(custom_system_prompt) if custom_system_prompt else 0
            }
            
            # Create WebRTC connection with ESP32 support - like the working examples
            from pipecat.transports.network.webrtc_connection import SmallWebRTCConnection
            from pipecat.runner.utils import smallwebrtc_sdp_munging
            from pipecat.runner.types import SmallWebRTCRunnerArguments
            
            # Create connection and get answer
            webrtc_connection = SmallWebRTCConnection()
            offer_sdp = body.get("sdp", "")
            offer_type = body.get("type", "offer")
            
            await webrtc_connection.initialize(offer_sdp, offer_type)
            answer = webrtc_connection.get_answer()
            
            # Apply ESP32 SDP munging for compatibility - CRUCIAL for ESP32!
            if ESP32_MODE and ESP32_HOST and ESP32_HOST not in ["localhost", "127.0.0.1", "0.0.0.0"]:
                logger.info(f"Applying ESP32 SDP munging for host: {ESP32_HOST}")
                answer["sdp"] = smallwebrtc_sdp_munging(answer["sdp"], ESP32_HOST)
            else:
                # Fallback: try to get host from environment or request
                host = request.headers.get("Host", "").split(":")[0]
                if not host or host in ["localhost", "127.0.0.1", "0.0.0.0"]:
                    host = os.getenv("SERVER_HOST", "64.227.157.74")
                
                if ESP32_MODE and host and host not in ["localhost", "127.0.0.1", "0.0.0.0"]:
                    logger.info(f"Applying ESP32 SDP munging for fallback host: {host}")
                    answer["sdp"] = smallwebrtc_sdp_munging(answer["sdp"], host)
                elif ESP32_MODE:
                    # If we're in ESP32 mode but don't have a valid host, disable SDP munging
                    logger.warning(f"ESP32 mode enabled but no valid host found (current: {host}). Skipping SDP munging.")
                    logger.info("Consider starting server with: --host 64.227.157.74 --esp32")
            
            # Create proper runner args for SmallWebRTC
            runner_args = SmallWebRTCRunnerArguments(webrtc_connection=webrtc_connection)
            runner_args.handle_sigint = False
            runner_args.pipeline_idle_timeout_secs = 60  # Increased timeout for better stability
            
            # Start the enhanced bot in background with custom prompt
            background_tasks.add_task(enhanced_bot_webrtc, runner_args, device_id, custom_system_prompt)
            
            # Update session status
            active_sessions[session_id]["status"] = "connected"
            
            logger.info(f"WebRTC connection established for device: {device_id}")
            if custom_system_prompt:
                logger.info(f"Using custom system prompt for enhanced AI personality")
            
            # Return the munged SDP answer for ESP32 compatibility
            return answer
            
        except Exception as e:
            logger.error(f"Error handling WebRTC offer: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    # WebRTC client info endpoint
    @app.get("/client-info",
             summary="WebRTC client info",
             description="Get WebRTC client connection information")
    async def webrtc_client_info():
        """Get WebRTC client information"""
        return {
            "webrtc_client": "/client",
            "offer_endpoint": "/api/offer",
            "instructions": "ESP32 devices should send WebRTC offers to /api/offer",
            "browser_client": "Open /client in browser for testing",
            "features": [
                "Interactive story sessions",
                "Automatic progress tracking", 
                "OpenAI function calling for story completion",
                "Season/episode advancement",
                "Learning analytics"
            ]
        }

    # Get user progress endpoint
    @app.get("/users/{device_id}",
             summary="Get user progress",
             description="Get user progress and learning data")
    async def get_user_progress(device_id: str):
        """Get user progress and learning data"""
        try:
            logger.info(f"üîç UPDATED ENDPOINT - Getting user progress for device_id: {device_id}")
            from services.firebase_service import get_firebase_service
            firebase_service = get_firebase_service()
            
            user_data = await firebase_service.get_document("users", device_id)
            if not user_data:
                logger.info(f"üÜï User {device_id} not found, returning default progress")
                # Return empty progress structure for non-existent users
                return {
                    "device_id": device_id,
                    "progress": {
                        "season": 1,
                        "episode": 1,
                        "episodes_completed": 0,
                        "words_learnt": [],
                        "topics_learnt": [],
                        "total_time_minutes": 0
                    },
                    "status": "new_user",
                    "message": "User not found in database, showing default progress"
                }
            
            logger.info(f"‚úÖ Found user data for {device_id}")
            return user_data
            
        except Exception as e:
            logger.error(f"Error getting user progress: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    
    # Manual story completion endpoint for testing
    @app.post("/api/complete-story",
              summary="Manual story completion",
              description="Manually complete a story episode for a user (testing only)")
    async def manual_story_completion(request: Request):
        """Manual story completion for testing purposes"""
        try:
            body = await request.json()
            device_id = body.get("device_id")
            words_learned = body.get("words_learned", [])
            topics_covered = body.get("topics_covered", [])
            time_spent_minutes = body.get("time_spent_minutes", 10)
            
            logger.info(f"üìö Manual story completion started for device: {device_id}")
            
            if not device_id:
                raise HTTPException(status_code=400, detail="device_id is required")
            
            from services.firebase_service import get_firebase_service
            firebase_service = get_firebase_service()
            
            # Get current user data from users collection (device_id based)
            logger.info(f"üîç Looking for user in users collection: {device_id}")
            current_user_data = await firebase_service.get_document("users", device_id)
            
            if not current_user_data:
                # If user doesn't exist in users collection, create a basic entry
                logger.info(f"‚ú® Creating basic user entry for device {device_id}")
                current_user_data = {
                    "device_id": device_id,
                    "progress": {
                        "season": 1,
                        "episode": 1,
                        "episodes_completed": 0,
                        "words_learnt": [],
                        "topics_learnt": [],
                        "total_time_minutes": 0
                    },
                    "created_at": datetime.now(timezone.utc).isoformat(),
                    "last_seen": datetime.now(timezone.utc).isoformat()
                }
                await firebase_service.set_document("users", device_id, current_user_data)
                logger.info(f"‚úÖ User created in users collection: {device_id}")
            else:
                logger.info(f"‚úÖ User found in users collection: {device_id}")
            
            logger.info(f"üìà Processing story completion data...")
            
            # Add new words (avoid duplicates)
            existing_words = set(current_user_data.get("progress", {}).get("words_learnt", []))
            new_words = [w for w in words_learned if w.lower() not in [ew.lower() for ew in existing_words]]
            all_words = list(existing_words) + new_words
            
            # Add new topics (avoid duplicates) 
            existing_topics = set(current_user_data.get("progress", {}).get("topics_learnt", []))
            new_topics = [t for t in topics_covered if t.lower() not in [et.lower() for et in existing_topics]]
            all_topics = list(existing_topics) + new_topics
            
            # Update episodes completed
            episodes_completed = current_user_data.get("progress", {}).get("episodes_completed", 0) + 1
            
            # Update user document
            updates = {
                "progress.words_learnt": all_words,
                "progress.topics_learnt": all_topics,
                "progress.episodes_completed": episodes_completed,
                "progress.total_time_minutes": current_user_data.get("progress", {}).get("total_time_minutes", 0) + time_spent_minutes,
                "last_seen": datetime.now(timezone.utc).isoformat()
            }
            
            logger.info(f"üíæ Updating user document with progress...")
            await firebase_service.update_document("users", device_id, updates)
            
            logger.info(f"‚úÖ Manual completion for {device_id}: +{len(new_words)} words, +{len(new_topics)} topics, +{time_spent_minutes} mins")
            
            return {
                "status": "success",
                "message": f"Episode completed for {device_id}",
                "new_words_added": len(new_words),
                "new_topics_added": len(new_topics),
                "total_words": len(all_words),
                "total_topics": len(all_topics),
                "episodes_completed": episodes_completed,
                "time_added_minutes": time_spent_minutes
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in manual story completion: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    
    # Advance user progress endpoint
    @app.post("/api/advance-progress",
              summary="Advance user progress",
              description="Advance user to next episode or season")
    async def advance_user_progress(request: Request):
        """Advance user to next episode or season"""
        try:
            body = await request.json()
            device_id = body.get("device_id")
            advance_type = body.get("advance_type", "next_episode")  # "next_episode" or "next_season"
            
            if not device_id:
                raise HTTPException(status_code=400, detail="device_id is required")
            
            from services.firebase_service import get_firebase_service
            firebase_service = get_firebase_service()
            
            current_user_data = await firebase_service.get_document("users", device_id)
            if not current_user_data:
                # Create basic user entry if doesn't exist
                logger.info(f"Creating basic user entry for device {device_id}")
                current_user_data = {
                    "device_id": device_id,
                    "progress": {
                        "season": 1,
                        "episode": 1,
                        "episodes_completed": 0,
                        "words_learnt": [],
                        "topics_learnt": [],
                        "total_time_minutes": 0
                    },
                    "created_at": datetime.now(timezone.utc).isoformat(),
                    "last_seen": datetime.now(timezone.utc).isoformat()
                }
                await firebase_service.set_document("users", device_id, current_user_data)
            
            current_season = current_user_data.get("progress", {}).get("season", 1)
            current_episode = current_user_data.get("progress", {}).get("episode", 1)
            
            if advance_type == "next_season":
                new_season = current_season + 1
                new_episode = 1
            else:  # next_episode
                new_season = current_season
                new_episode = current_episode + 1
            
            # Update progress
            updates = {
                "progress.season": new_season,
                "progress.episode": new_episode,
                "last_seen": datetime.now(timezone.utc).isoformat()
            }
            
            await firebase_service.update_document("users", device_id, updates)
            
            logger.info(f"Advanced user {device_id} from S{current_season}E{current_episode} to S{new_season}E{new_episode}")
            
            return {
                "status": "success",
                "message": f"Advanced {device_id} to Season {new_season}, Episode {new_episode}",
                "previous": {"season": current_season, "episode": current_episode},
                "current": {"season": new_season, "episode": new_episode},
                "advance_type": advance_type
            }
            
        except Exception as e:
            logger.error(f"Error advancing user progress: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    
    return app

async def run_enhanced_bot(transport: BaseTransport, runner_args: RunnerArguments, device_id: str = None, custom_system_prompt: str = None):
    """Enhanced bot function - exactly like 07-interruptible.py but with Firebase integration and custom prompts"""
    logger.info(f"Starting enhanced bot for device: {device_id}")
    if custom_system_prompt:
        logger.info(f"Using custom system prompt ({len(custom_system_prompt)} characters)")
    
    conversation_id = None
    user = None
    firebase_service = None
    conversation_start_time = datetime.now(timezone.utc)
    
    # Create an HTTP session for ElevenLabs TTS
    async with aiohttp.ClientSession() as session:
        try:
            # Initialize conversation tracking if device_id is provided
            if device_id:
                from services.firebase_service import get_firebase_service
                from services.enhanced_user_service import EnhancedUserService
                from services.conversation_service import ConversationService
            
                firebase_service = get_firebase_service()
                user_service = EnhancedUserService(firebase_service)
                conversation_service = ConversationService(firebase_service)
                
                # Get user and start conversation session
                user = await user_service.get_user_by_device_id(device_id)
                if user:
                    # Start a conversation session
                    conversation_id = f"{user.email}_{user.progress.season}_{user.progress.episode}_{int(datetime.now(timezone.utc).timestamp())}"
                    
                    from models.conversation import ConversationTranscript
                    transcript = ConversationTranscript(
                        conversation_id=conversation_id,
                        user_email=user.email,
                        device_id=device_id,
                        start_time=conversation_start_time,
                        season=user.progress.season,
                        episode=user.progress.episode,
                        user_age=user.age,
                        user_name=user.name,
                        messages=[],
                        story_completions=[],
                        final_summary=""
                    )
                    
                    await conversation_service.create_conversation(transcript)
                    logger.info(f"Started conversation session {conversation_id} for user {user.email}")

            # Services - exactly like 07-interruptible.py
            stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

            # Debug: Check if API key is loaded
            elevenlabs_api_key = os.getenv("ELEVENLABS_API_KEY")
            elevenlabs_voice_id = os.getenv("ELEVENLABS_VOICE_ID", "")
            logger.info(f"ElevenLabs API key loaded: {elevenlabs_api_key[:10] if elevenlabs_api_key else 'None'}...")
            logger.info(f"ElevenLabs Voice ID: {elevenlabs_voice_id}")

            tts = ElevenLabsHttpTTSService(
                api_key=elevenlabs_api_key,
                voice_id=elevenlabs_voice_id,
                aiohttp_session=session,
            )

            # Enhanced LLM with function calling for story completion
            llm = OpenAILLMService(
                api_key=os.getenv("OPENAI_API_KEY"),
                model="gpt-4",  # Use GPT-4 for better function calling
            )

            # Define story completion functions for OpenAI to call
            story_completion_functions = [
            {
                "type": "function",
                "function": {
                    "name": "complete_story_episode",
                    "description": "Call this when the interactive story/episode has been completed by the user. This will update their learning progress.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "words_learned": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "List of new words the user learned during this story episode"
                            },
                            "topics_covered": {
                                "type": "array", 
                                "items": {"type": "string"},
                                "description": "List of topics/themes covered in this story episode"
                            },
                            "story_completed": {
                                "type": "boolean",
                                "description": "Whether the user successfully completed the entire story episode"
                            },
                            "user_engagement_level": {
                                "type": "string",
                                "enum": ["low", "medium", "high"],
                                "description": "How engaged the user was during the story (based on their responses and participation)"
                            },
                            "completion_summary": {
                                "type": "string",
                                "description": "Brief summary of what the user accomplished in this episode"
                            }
                        },
                        "required": ["words_learned", "topics_covered", "story_completed", "user_engagement_level", "completion_summary"]
                    }
                }
            },
            {
                "type": "function", 
                "function": {
                    "name": "advance_to_next_episode",
                    "description": "Call this to advance the user to the next episode or season after they complete a story",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "advance_type": {
                                "type": "string",
                                "enum": ["next_episode", "next_season"],
                                "description": "Whether to advance to next episode in same season or next season"
                            },
                            "reason": {
                                "type": "string",
                                "description": "Reason for advancement (e.g., 'completed current episode successfully')"
                            }
                        },
                        "required": ["advance_type", "reason"]
                    }
                }
            }
        ]

        # Use custom system prompt if provided, otherwise get enhanced prompt
        if custom_system_prompt:
            # Enhance custom prompts with story completion instructions
            system_prompt = f"""{custom_system_prompt}

IMPORTANT STORY COMPLETION INSTRUCTIONS:
- You are conducting an interactive story session
- Pay attention to when the user has completed the story/episode
- When the story naturally concludes, call the 'complete_story_episode' function with:
  * Words the user learned during the conversation
  * Topics/themes covered in the story
  * Whether they completed it successfully
  * Their engagement level (low/medium/high)
  * A brief summary of their accomplishment
- After completion, if appropriate, call 'advance_to_next_episode' to progress them
- Make the story interactive - ask questions, get user responses, guide them through the narrative
- Keep responses conversational since they will be spoken aloud"""
            logger.info(f"Using enhanced custom system prompt for interactive story")
        else:
            system_prompt = await get_enhanced_system_prompt(device_id)
            # Add story completion instructions to Firebase prompts too
            system_prompt += """

IMPORTANT STORY COMPLETION INSTRUCTIONS:
- You are conducting an interactive story/lesson session
- Pay attention to when the user has completed the story/episode/lesson
- When the session naturally concludes, call the 'complete_story_episode' function with learning data
- After completion, if appropriate, call 'advance_to_next_episode' to progress them
- Make the experience interactive and engaging"""
            logger.info("Using enhanced Firebase system prompt with story completion")
        
        messages = [
            {
                "role": "system",
                "content": system_prompt,
            },
        ]

        context = OpenAILLMContext(messages, tools=story_completion_functions)
        context_aggregator = llm.create_context_aggregator(context)

        # Function to handle story completion calls
        async def handle_story_completion(function_name: str, tool_call_id: str, arguments: dict):
            """Handle function calls from OpenAI for story completion"""
            logger.info(f"Story completion function called: {function_name} with args: {arguments}")
            
            try:
                if function_name == "complete_story_episode":
                    if user and firebase_service:
                        # Calculate time spent
                        time_spent_minutes = (datetime.now(timezone.utc) - conversation_start_time).total_seconds() / 60
                        
                        # Update user progress
                        words_learned = arguments.get("words_learned", [])
                        topics_covered = arguments.get("topics_covered", [])
                        
                        # Get current user data
                        current_user_data = await firebase_service.get_document("users", device_id)
                        if not current_user_data:
                            # Create basic user entry if doesn't exist
                            logger.info(f"Creating basic user entry for device {device_id}")
                            current_user_data = {
                                "device_id": device_id,
                                "progress": {
                                    "season": 1,
                                    "episode": 1,
                                    "episodes_completed": 0,
                                    "words_learnt": [],
                                    "topics_learnt": [],
                                    "total_time_minutes": 0
                                },
                                "created_at": datetime.now(timezone.utc).isoformat(),
                                "last_seen": datetime.now(timezone.utc).isoformat()
                            }
                            await firebase_service.set_document("users", device_id, current_user_data)
                        
                        # Add new words (avoid duplicates)
                        existing_words = set(current_user_data.get("progress", {}).get("words_learnt", []))
                        new_words = [w for w in words_learned if w.lower() not in [ew.lower() for ew in existing_words]]
                        all_words = list(existing_words) + new_words
                        
                        # Add new topics (avoid duplicates) 
                        existing_topics = set(current_user_data.get("progress", {}).get("topics_learnt", []))
                        new_topics = [t for t in topics_covered if t.lower() not in [et.lower() for et in existing_topics]]
                        all_topics = list(existing_topics) + new_topics
                        
                        # Update episodes completed
                        episodes_completed = current_user_data.get("progress", {}).get("episodes_completed", 0) + 1
                        
                        # Update user document
                        updates = {
                            "progress.words_learnt": all_words,
                            "progress.topics_learnt": all_topics,
                            "progress.episodes_completed": episodes_completed,
                            "progress.total_time_minutes": current_user_data.get("progress", {}).get("total_time_minutes", 0) + time_spent_minutes,
                            "last_seen": datetime.now(timezone.utc).isoformat()
                        }
                        
                        await firebase_service.update_document("users", device_id, updates)
                        
                        logger.info(f"Updated user {device_id}: +{len(new_words)} words, +{len(new_topics)} topics, +{time_spent_minutes:.1f} mins")
                        
                        # Log completion to conversation transcript
                        if conversation_id:
                            completion_data = {
                                "episode_completed": True,
                                "completion_time": datetime.now(timezone.utc).isoformat(),
                                "time_spent_minutes": time_spent_minutes,
                                "words_learned": words_learned,
                                "topics_covered": topics_covered,
                                "user_engagement_level": arguments.get("user_engagement_level", "medium"),
                                "completion_summary": arguments.get("completion_summary", ""),
                                "story_completed": arguments.get("story_completed", True)
                            }
                            await firebase_service.update_document("conversation_transcripts", conversation_id, completion_data)
                    
                    return {
                        "status": "success",
                        "message": f"Episode completed! Learned {len(arguments.get('words_learned', []))} new words and {len(arguments.get('topics_covered', []))} topics.",
                        "words_added": len(arguments.get('words_learned', [])),
                        "topics_added": len(arguments.get('topics_covered', []))
                    }
                
                elif function_name == "advance_to_next_episode":
                    if user and firebase_service:
                        current_user_data = await firebase_service.get_document("users", device_id)
                        if not current_user_data:
                            # Create basic user entry if doesn't exist
                            logger.info(f"Creating basic user entry for device {device_id}")
                            current_user_data = {
                                "device_id": device_id,
                                "progress": {
                                    "season": 1,
                                    "episode": 1,
                                    "episodes_completed": 0,
                                    "words_learnt": [],
                                    "topics_learnt": [],
                                    "total_time_minutes": 0
                                },
                                "created_at": datetime.now(timezone.utc).isoformat(),
                                "last_seen": datetime.now(timezone.utc).isoformat()
                            }
                            await firebase_service.set_document("users", device_id, current_user_data)
                        
                        current_season = current_user_data.get("progress", {}).get("season", 1)
                        current_episode = current_user_data.get("progress", {}).get("episode", 1)
                        
                        if arguments.get("advance_type") == "next_season":
                            new_season = current_season + 1
                            new_episode = 1
                        else:  # next_episode
                            new_season = current_season
                            new_episode = current_episode + 1
                        
                        # Update progress
                        updates = {
                            "progress.season": new_season,
                            "progress.episode": new_episode,
                            "last_seen": datetime.now(timezone.utc).isoformat()
                        }
                        
                        await firebase_service.update_document("users", device_id, updates)
                        
                        logger.info(f"Advanced user {device_id} to Season {new_season}, Episode {new_episode}")
                    
                    return {
                        "status": "success", 
                        "message": f"Advanced to {arguments.get('advance_type', 'next episode')}!",
                        "reason": arguments.get("reason", "Story completed")
                    }
                
            except Exception as e:
                logger.error(f"Error handling story completion: {e}")
                return {"status": "error", "message": str(e)}

        # Pipeline - exactly like 07-interruptible.py but with function calling support
        pipeline = Pipeline(
            [
                transport.input(),  # Transport user input
                stt,
                context_aggregator.user(),  # User responses
                llm,  # LLM with function calling
                tts,  # TTS
                transport.output(),  # Transport bot output
                context_aggregator.assistant(),  # Assistant spoken responses
            ]
        )

        task = PipelineTask(
            pipeline,
            params=PipelineParams(
                enable_metrics=True,
                enable_usage_metrics=True,
            ),
            idle_timeout_secs=runner_args.pipeline_idle_timeout_secs,
        )

        @transport.event_handler("on_client_connected")
        async def on_client_connected(transport, client):
            logger.info(f"Client connected - device: {device_id}")
            # Update user last seen if device_id provided  
            if device_id:
                try:
                    # For now, just log the connection - user service integration can be added later
                    logger.info(f"Device {device_id} connected and active")
                except Exception as e:
                    logger.warning(f"Failed to update last seen for {device_id}: {e}")
            
            # Kick off the conversation - exactly like 07-interruptible.py
            if custom_system_prompt:
                # For custom prompts, give a more generic introduction
                messages.append({"role": "system", "content": "Please introduce yourself to the user according to your role and start the interactive story session."})
            else:
                # Use the standard introduction for Firebase users
                messages.append({"role": "system", "content": "Please introduce yourself to the user and begin their learning episode."})
            
            await task.queue_frames([context_aggregator.user().get_context_frame()])

        @transport.event_handler("on_client_disconnected")
        async def on_client_disconnected(transport, client):
            logger.info(f"Client disconnected - device: {device_id}")
            
            # Finalize conversation session if it exists
            if device_id and conversation_id and firebase_service:
                try:
                    # Get conversation transcript and create summary
                    transcript_doc = await firebase_service.get_document("conversation_transcripts", conversation_id)
                    if transcript_doc:
                        conversation_messages = []
                        for msg in messages[1:]:  # Skip system prompt
                            conversation_messages.append(msg)
                        
                        # Calculate final session time
                        final_time_spent = (datetime.now(timezone.utc) - conversation_start_time).total_seconds() / 60
                        
                        # Update transcript with conversation messages
                        transcript_doc['messages'] = conversation_messages
                        transcript_doc['ended_at'] = datetime.now(timezone.utc).isoformat()
                        transcript_doc['custom_prompt_used'] = bool(custom_system_prompt)
                        transcript_doc['total_session_time_minutes'] = final_time_spent
                        
                        await firebase_service.update_document("conversation_transcripts", conversation_id, transcript_doc)
                        
                        # Update user progress if appropriate
                        if user:
                            logger.info(f"Conversation {conversation_id} completed for user {user.email} - Duration: {final_time_spent:.1f} mins")
                            
                except Exception as e:
                    logger.error(f"Failed to finalize conversation {conversation_id}: {e}")
            
            # Clean up session
            if device_id and device_id in active_sessions:
                del active_sessions[device_id]
            if device_id and device_id in active_transports:
                del active_transports[device_id]
            await task.cancel()

            runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)
            await runner.run(task)
            
        except Exception as e:
            logger.error(f"Error in enhanced bot: {e}")
            if device_id and device_id in active_sessions:
                del active_sessions[device_id]
            if device_id and device_id in active_transports:
                del active_transports[device_id]
            raise

async def enhanced_bot(runner_args: RunnerArguments, device_id: str = None, custom_system_prompt: str = None):
    """Enhanced bot entry point - like 07-interruptible.py but with device_id and custom prompts"""
    transport = await create_transport(runner_args, transport_params)
    active_transports[device_id or "default"] = transport
    await run_enhanced_bot(transport, runner_args, device_id, custom_system_prompt)

async def enhanced_bot_webrtc(runner_args: SmallWebRTCRunnerArguments, device_id: str = None, custom_system_prompt: str = None):
    """Enhanced bot entry point for WebRTC connections with Firebase integration and custom prompts"""
    from pipecat.transports.network.small_webrtc import SmallWebRTCTransport
    from pipecat.runner.utils import _get_transport_params
    
    # Get transport params for webrtc - exactly like working examples
    transport_params_obj = _get_transport_params("webrtc", transport_params)
    
    # Create WebRTC transport from connection
    transport = SmallWebRTCTransport(runner_args.webrtc_connection, transport_params_obj)
    
    # Store in active transports
    active_transports[device_id or "default"] = transport
    
    # Run the enhanced bot with WebRTC transport and custom prompt
    await run_enhanced_bot(transport, runner_args, device_id, custom_system_prompt)

# Create the enhanced app
app = create_enhanced_app()

# Global ESP32 mode flag
ESP32_MODE = False
ESP32_HOST = None

def main():
    global ESP32_MODE, ESP32_HOST
    
    parser = argparse.ArgumentParser(description="Enhanced Pipecat Server - 07-interruptible.py Compatible")
    parser.add_argument("--host", default="127.0.0.1", help="Host to bind to")
    parser.add_argument("--port", type=int, default=7860, help="Port to bind to")
    parser.add_argument("--reload", action="store_true", help="Enable auto-reload")
    parser.add_argument("--log-level", default="info", help="Log level")
    parser.add_argument("--esp32", action="store_true", help="Enable ESP32 mode with SDP munging")
    
    args = parser.parse_args()
    
    # Set global ESP32 mode
    ESP32_MODE = args.esp32
    ESP32_HOST = args.host
    
    # Validate ESP32 requirements
    if args.esp32 and args.host in ["localhost", "127.0.0.1"]:
        logger.error("For ESP32, you need to specify `--host IP` so we can do SDP munging.")
        sys.exit(1)
    
    print(f"üöÄ Starting Enhanced Pipecat Server (07-interruptible.py compatible)...")
    print(f"   Host: {args.host}")
    print(f"   Port: {args.port}")
    print(f"   Log Level: {args.log_level}")
    print(f"   Reload: {args.reload}")
    if args.esp32:
        print(f"ü§ñ ESP32 mode enabled with SDP munging for host: {args.host}")
    
    # Run the server directly
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level=args.log_level,
        reload=args.reload,
        access_log=True
    )

if __name__ == "__main__":
    main()
